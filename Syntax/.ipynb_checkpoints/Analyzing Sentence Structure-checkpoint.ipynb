{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тексты для синтаксической разметки разметки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6046742\n",
      "Тема информационных войн сегодня актуальна, вошла в политическую моду и народный обиход. Вспоминаетс\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "text = ' '\n",
    "for i in pd.read_csv('data_text.csv').content.tolist():\n",
    "    text+=i+' '\n",
    "example = i\n",
    "print(len(text))#длина в чем измеряется в данном случае?\n",
    "print(example[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Воспользуемся пайморфи, чтобы расставить теги для каждого токена, но перед этим поделим на предложения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/semen/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Тема информационных войн сегодня актуальна, вошла в политическую моду и народный обиход.',\n",
       " 'Вспоминается, как уже в далеком 1994 году при моем участии в Совете безопасности была создана межведомственная рабочая группа по информационной безопасности, которую я курировал как заместитель секретаря.',\n",
       " 'Вскоре после организации группы в одной из популярных аналитических программ был примечательный сюжет.',\n",
       " 'В нем обсуждалась моя персона с недоуменным вопросом: «А чем занимается в Совбезе господин Рубанов, когда у страны столько угроз?',\n",
       " 'Какой-то информационной безопасностью!»\\nС этого все начиналось.',\n",
       " 'А сегодня термин «информационная безопасность» стал общеупотребимым и дополнил собой тематический ряд из медицины и футбола, в которых разбираются все.',\n",
       " 'Сразу хочу подчеркнуть, что проблемы, с решения которых начиналось обеспечение информационной безопасности России, касались радиоэлектронной борьбы, скрытного программно-аппаратного воздействия на информационные системы, контроля поставок компьютерной техники и программ на предмет недекларированных возможностей применения.',\n",
       " 'Тогда же государственным руководством была правильно идентифицирована и удачно решена проблема организации и использования на территории Российской Федерации средств криптографии в информационных системах (было разрешено применение только отечественных средств).',\n",
       " 'Это помогло не только успешно решить проблему технологической независимости по защите секретной информации, но и сохранить собственную научную школу в этой сфере, продолжить развитие уникального интеллектуально-кадрового потенциала мирового уровня.',\n",
       " 'Сегодня информационная безопасность с изначальным содержанием стала предметом заботы, работы и интереса профессионального сообщества.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "sent_text = nltk.sent_tokenize(example)\n",
    "sent_text[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Тема', 'информационных', 'войн', 'сегодня', 'актуальна', 'вошла', 'в', 'политическую', 'моду', 'и', 'народный', 'обиход']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "tokenized_sent_text = []\n",
    "for sent in sent_text:\n",
    "    new_sent = re.findall('[а-яА-ЯёЁ]+', sent)\n",
    "    tokenized_sent_text.append(new_sent)\n",
    "print(tokenized_sent_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Минутка омонимии.\n",
    "Трактор тащит экскаватор - Экскаватор тащит трактор.\n",
    "<p>С наблюдай ее во дворе в телескоп в пижаме.\n",
    "<p>I shot an elephant in my pajamas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Самое грубое допущение!!!\n",
    "Представим, что каждое следующее слово зависит от предыдущего, и что именно так строятся синтаксические деревья.\n",
    "Приведите примеры когда, это такое приближение верно, а когда оно опровергается. Давайте проверим гипотезу статистически."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Исходя из нашего предположения давай построим частотный словарь, в котором содержится (ТЕГ предыдущего слова, ТЕГ следующего слова) и их относительная частота."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NOUN', 'ADJF', 'NOUN', 'ADVB', 'ADJS', 'VERB', 'PREP', 'ADJF', 'NOUN', 'CONJ', 'ADJF', 'NOUN'] 1892\n"
     ]
    }
   ],
   "source": [
    "n_words=0\n",
    "sents_tagged = []\n",
    "for sent in  tokenized_sent_text:\n",
    "    sent_tag = []\n",
    "    for word in sent:\n",
    "        sent_tag.append(morph.parse(word)[0].tag.POS)\n",
    "        n_words+=1\n",
    "    sents_tagged.append(sent_tag)\n",
    "print(sents_tagged[0], n_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведем самые частотные пары тегов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('ADJF', 'NOUN'), 279),\n",
       " (('NOUN', 'NOUN'), 161),\n",
       " (('NOUN', 'ADJF'), 127),\n",
       " (('PREP', 'NOUN'), 107),\n",
       " (('NOUN', 'PREP'), 102),\n",
       " (('NOUN', 'CONJ'), 92),\n",
       " (('PREP', 'ADJF'), 75),\n",
       " (('NOUN', 'VERB'), 65),\n",
       " (('CONJ', 'NOUN'), 58),\n",
       " (('CONJ', 'ADJF'), 33)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_count={}\n",
    "#ваш код здесь\n",
    "#\n",
    "#\n",
    "#\n",
    "#примерно тут должен кончиться, но это не точно \n",
    "a = list(dict_count.items())\n",
    "a.sort(key = lambda x:x[1])\n",
    "a[:-11:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Абсолютная и относительная величина. Какая представлена в нашем словаре?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('NOUN', 'NOUN'), 89990),\n",
       " (('ADJF', 'NOUN'), 80975),\n",
       " (('PREP', 'NOUN'), 59809),\n",
       " (('NOUN', 'PREP'), 45302),\n",
       " (('NOUN', 'ADJF'), 34478),\n",
       " (('NOUN', 'VERB'), 31123),\n",
       " (('NOUN', 'CONJ'), 29499),\n",
       " (('PREP', 'ADJF'), 27207),\n",
       " (('CONJ', 'NOUN'), 18634),\n",
       " (('VERB', 'NOUN'), 16536),\n",
       " (('VERB', 'PREP'), 12753),\n",
       " (('ADJF', 'ADJF'), 11999),\n",
       " (('NOUN', 'ADVB'), 10437),\n",
       " (('CONJ', 'ADJF'), 10403),\n",
       " (('VERB', 'ADJF'), 9004),\n",
       " (('CONJ', 'PREP'), 7906),\n",
       " (('PRCL', 'VERB'), 7660),\n",
       " (('VERB', 'CONJ'), 6895),\n",
       " (('NPRO', 'VERB'), 6797)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_freq = a[:-20:-1]\n",
    "top_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_freq_correct = [(('NOUN', 'NOUN'), 89990),\n",
    " (('PREP', 'NOUN'), 59809),\n",
    " (('NOUN', 'ADJF'), 34478),\n",
    " (('NOUN', 'VERB'), 31123),\n",
    " (('NOUN', 'CONJ'), 29499),\n",
    " (('VERB', 'NOUN'), 16536),\n",
    " (('VERB', 'PREP'), 12753),\n",
    " (('NOUN', 'ADVB'), 10437),\n",
    " (('NPRO', 'VERB'), 6797)]\n",
    "top_freq_pairs = [a[0] for a in top_freq_correct]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "exm = tokenized_sent_text[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Побить на биграммы слова и теги, каждой паре тегов присвоить индекс, для дальнейшего вызова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Основной', 'задачей'), ('задачей', 'Молодежного'), ('Молодежного', 'Яблока'), ('Яблока', 'является'), ('является', 'привлечение'), ('привлечение', 'молодых'), ('молодых', 'людей'), ('людей', 'к'), ('к', 'участию'), ('участию', 'в'), ('в', 'выборах'), ('выборах', 'и'), ('и', 'деятельности'), ('деятельности', 'партии')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(('ADJF', 'NOUN'), 0),\n",
       " (('NOUN', 'ADJF'), 1),\n",
       " (('ADJF', 'NOUN'), 2),\n",
       " (('NOUN', 'VERB'), 3),\n",
       " (('VERB', 'NOUN'), 4),\n",
       " (('NOUN', 'NOUN'), 5),\n",
       " (('NOUN', 'NOUN'), 6),\n",
       " (('NOUN', 'PREP'), 7),\n",
       " (('PREP', 'NOUN'), 8),\n",
       " (('NOUN', 'PREP'), 9),\n",
       " (('PREP', 'NOUN'), 10),\n",
       " (('NOUN', 'CONJ'), 11),\n",
       " (('CONJ', 'NOUN'), 12),\n",
       " (('NOUN', 'NOUN'), 13)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exm_tag = []\n",
    "for word in exm:\n",
    "    exm_tag.append(morph.parse(word)[0].tag.POS)\n",
    "    \n",
    "exm_tag_pairs = []\n",
    "exm_pairs  = []\n",
    "\n",
    "#код и котики тут"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('задачей', 'Молодежного')\n",
      "('Яблока', 'является')\n",
      "('является', 'привлечение')\n",
      "('привлечение', 'молодых')\n",
      "('молодых', 'людей')\n",
      "('к', 'участию')\n",
      "('в', 'выборах')\n",
      "('выборах', 'и')\n",
      "('деятельности', 'партии')\n"
     ]
    }
   ],
   "source": [
    "for pair, ind in exm_tag_pairs:\n",
    "    if pair in top_freq_pairs:\n",
    "        print(exm_pairs[ind])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь у нас есть отдельные кусочки нашего дерева синтаксических зависимостей, еще несколько \"трюков ушами\" и можно будет построить небольшое дерево, которое будет частично \"угадывать\" настоящую структуру предложения.\n",
    "Давайте обсудим, в чем минусы такого подхода, и для какого языка такой словарь частот зависимотей между тегами работал бы с более выской точность. (Подсказка - фиксированный порядок слов)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на настоящие системы построения синтаксичских деревьев. Одна из них представлена в библиотеке ***spicy***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple apple PROPN NNP nsubj Xxxxx True False\n",
      "is be VERB VBZ aux xx True False\n",
      "looking look VERB VBG ROOT xxxx True False\n",
      "at at ADP IN prep xx True False\n",
      "buying buy VERB VBG pcomp xxxx True False\n",
      "U.K. u.k. PROPN NNP compound X.X. False False\n",
      "startup startup NOUN NN dobj xxxx True False\n",
      "for for ADP IN prep xxx True False\n",
      "$ $ SYM $ quantmod $ False False\n",
      "1 1 NUM CD compound d False False\n",
      "billion billion NUM CD pobj xxxx True False\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "doc = nlp(u'Apple is looking at buying U.K. startup for $1 billion')\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "          token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[93m    Serving on port 5000...\u001b[0m\n",
      "    Using the 'dep' visualizer\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [06/Feb/2019 15:13:27] \"GET /notebooks/syntax.ipynb HTTP/1.1\" 200 5366\n",
      "127.0.0.1 - - [06/Feb/2019 15:13:28] \"GET /favicon.ico HTTP/1.1\" 200 5366\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Shutting down server on port 5000.\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "doc = nlp(u'I shot an elephant in my pajamas')\n",
    "print(displacy.serve(doc, style='dep'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./s_tree.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./pajamas_tree.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задача №1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in sents_tagged:\n",
    "    for indx in range(len(sent)-1):\n",
    "        if (sent[indx], sent[indx+1]) not in dict_count:\n",
    "            dict_count[(sent[indx], sent[indx+1])]=1\n",
    "        else:\n",
    "            dict_count[(sent[indx], sent[indx+1])]+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задача №2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for indx in range(len(exm)-1):\n",
    "    exm_pairs.append((exm[indx], exm[indx+1]))\n",
    "print(exm_pairs)\n",
    "\n",
    "\n",
    "for indx in range(len(exm_tag)-1):\n",
    "    exm_tag_pairs.append(((exm_tag[indx], exm_tag[indx+1]),indx ))\n",
    "exm_tag_pairs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
